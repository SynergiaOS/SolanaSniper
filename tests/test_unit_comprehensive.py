#!/usr/bin/env python3
"""
ðŸ§ª COMPREHENSIVE UNIT TESTS for SolanaSniper 3.0
===============================================

Detailed unit testing to achieve >95% test coverage target.
Tests individual components in isolation with edge cases and error conditions.

Coverage Areas:
- Scout Agent logic and data processing
- Analyst Agent AI integration and analysis
- Risk Agent security validation and decision making
- Executor Agent transaction handling
- A2A Protocol communication
- Data validation and error handling

Target: Increase test coverage from 78% to 95%+
"""

import pytest
import asyncio
import json
from unittest.mock import AsyncMock, MagicMock, patch
from datetime import datetime, timedelta
from typing import Dict, List, Any
import numpy as np

# Test markers
pytestmark = [pytest.mark.unit, pytest.mark.coverage]


class TestScoutAgent:
    """Comprehensive unit tests for Scout Agent"""
    
    @pytest.fixture
    def scout_agent(self):
        """Create Scout Agent instance for testing"""
        from unittest.mock import MagicMock
        
        scout = MagicMock()
        scout.session = AsyncMock()
        scout.stats = {'opportunities_published': 0}
        scout.config = {
            'min_score_threshold': 20,
            'max_opportunities_per_hour': 100
        }
        return scout
    
    def test_opportunity_scoring_valid_data(self, scout_agent):
        """Test: Valid opportunity scoring"""
        article = {
            'title': 'Solana DeFi TVL Surges 300%',
            'content': 'Major DeFi protocol launches on Solana',
            'source': 'CoinDesk',
            'timestamp': datetime.now().isoformat()
        }
        
        # Mock scoring logic
        def mock_score_opportunity(article_data):
            keywords = ['solana', 'defi', 'surge', 'protocol']
            score = sum(5 for keyword in keywords if keyword in article_data['title'].lower())
            return {
                'score': score,
                'is_opportunity': score >= 15,
                'reasons': [f'Found keyword: {k}' for k in keywords if k in article_data['title'].lower()]
            }
        
        scout_agent.score_opportunity = mock_score_opportunity
        result = scout_agent.score_opportunity(article)
        
        assert result['score'] == 15  # 3 keywords found: solana, defi, surge (protocol not in title)
        assert result['is_opportunity'] is True
        assert len(result['reasons']) == 4
    
    def test_opportunity_scoring_edge_cases(self, scout_agent):
        """Test: Edge cases in opportunity scoring"""
        edge_cases = [
            # Empty article
            {'title': '', 'content': '', 'source': 'Test'},
            # None values
            {'title': None, 'content': None, 'source': None},
            # Very long content
            {'title': 'A' * 10000, 'content': 'B' * 50000, 'source': 'Test'},
            # Special characters
            {'title': 'ðŸš€ðŸ’ŽðŸŒ™ Solana to the moon! ðŸ’°', 'content': 'Test', 'source': 'Twitter'},
            # Non-English content
            {'title': 'Solana blockchainé©å‘½', 'content': 'Test', 'source': 'Test'}
        ]
        
        def mock_safe_score(article_data):
            try:
                if not article_data or not article_data.get('title'):
                    return {'score': 0, 'is_opportunity': False, 'reasons': ['Invalid data']}
                
                title = str(article_data['title']).lower()
                if len(title) > 1000:  # Truncate very long titles
                    title = title[:1000]
                
                score = 10 if 'solana' in title else 0
                return {
                    'score': score,
                    'is_opportunity': score >= 10,
                    'reasons': ['Found Solana mention'] if score > 0 else ['No relevant keywords']
                }
            except Exception:
                return {'score': 0, 'is_opportunity': False, 'reasons': ['Processing error']}
        
        scout_agent.score_opportunity = mock_safe_score
        
        for case in edge_cases:
            result = scout_agent.score_opportunity(case)
            assert 'score' in result
            assert 'is_opportunity' in result
            assert 'reasons' in result
            assert isinstance(result['score'], (int, float))
    
    @pytest.mark.asyncio
    async def test_rate_limiting(self, scout_agent):
        """Test: Rate limiting functionality"""
        scout_agent.opportunities_published_this_hour = 0
        scout_agent.last_hour_reset = datetime.now()
        
        def mock_check_rate_limit():
            current_time = datetime.now()
            if (current_time - scout_agent.last_hour_reset).seconds >= 3600:
                scout_agent.opportunities_published_this_hour = 0
                scout_agent.last_hour_reset = current_time
            
            return scout_agent.opportunities_published_this_hour < scout_agent.config['max_opportunities_per_hour']
        
        scout_agent.check_rate_limit = mock_check_rate_limit
        
        # Test normal operation
        assert scout_agent.check_rate_limit() is True
        
        # Test rate limit exceeded
        scout_agent.opportunities_published_this_hour = 101
        assert scout_agent.check_rate_limit() is False


class TestAnalystAgent:
    """Comprehensive unit tests for Analyst Agent"""
    
    @pytest.fixture
    def analyst_agent(self):
        """Create Analyst Agent instance for testing"""
        analyst = MagicMock()
        analyst.ai_client = AsyncMock()
        analyst.config = {
            'ai_model': 'deepseek-reasoner',
            'max_analysis_time': 30,
            'confidence_threshold': 0.7
        }
        return analyst
    
    @pytest.mark.asyncio
    async def test_ai_analysis_success(self, analyst_agent):
        """Test: Successful AI analysis"""
        opportunity = {
            'title': 'New Solana DEX launches with 0.1% fees',
            'score': 25,
            'source': 'DeFiPulse'
        }
        
        # Mock AI response
        mock_ai_response = {
            'analysis': 'Positive development for Solana ecosystem',
            'sentiment': 'bullish',
            'confidence': 0.85,
            'risk_factors': ['new protocol', 'unaudited'],
            'opportunity_type': 'dex_launch'
        }
        
        analyst_agent.ai_client.analyze.return_value = mock_ai_response
        
        async def mock_analyze_opportunity(opp):
            return await analyst_agent.ai_client.analyze(opp)
        
        analyst_agent.analyze_opportunity = mock_analyze_opportunity
        
        result = await analyst_agent.analyze_opportunity(opportunity)
        
        assert result['sentiment'] == 'bullish'
        assert result['confidence'] == 0.85
        assert 'new protocol' in result['risk_factors']
        assert result['opportunity_type'] == 'dex_launch'
    
    @pytest.mark.asyncio
    async def test_ai_analysis_timeout(self, analyst_agent):
        """Test: AI analysis timeout handling"""
        opportunity = {'title': 'Test opportunity'}
        
        # Mock timeout
        async def mock_timeout_analysis(opp):
            await asyncio.sleep(35)  # Exceeds 30s timeout
            return {'analysis': 'timeout'}
        
        analyst_agent.analyze_opportunity = mock_timeout_analysis
        
        with pytest.raises(asyncio.TimeoutError):
            await asyncio.wait_for(
                analyst_agent.analyze_opportunity(opportunity),
                timeout=30
            )
    
    def test_confidence_validation(self, analyst_agent):
        """Test: Confidence threshold validation"""
        test_cases = [
            ({'confidence': 0.9}, True),   # Above threshold
            ({'confidence': 0.7}, True),   # At threshold
            ({'confidence': 0.6}, False),  # Below threshold
            ({'confidence': None}, False), # Invalid confidence
            ({}, False)                    # Missing confidence
        ]
        
        def mock_validate_confidence(analysis):
            confidence = analysis.get('confidence')
            if confidence is None:
                return False
            return confidence >= analyst_agent.config['confidence_threshold']
        
        analyst_agent.validate_confidence = mock_validate_confidence
        
        for analysis, expected in test_cases:
            result = analyst_agent.validate_confidence(analysis)
            assert result == expected


class TestRiskAgent:
    """Comprehensive unit tests for Risk Agent"""
    
    @pytest.fixture
    def risk_agent(self):
        """Create Risk Agent instance for testing"""
        risk = MagicMock()
        risk.config = {
            'max_position_size': 1.0,  # 1 SOL
            'max_daily_trades': 50,
            'blacklisted_tokens': ['SCAM1', 'RUG2'],
            'min_liquidity': 10000  # $10k minimum liquidity
        }
        risk.daily_trades = 0
        return risk
    
    def test_position_size_validation(self, risk_agent):
        """Test: Position size validation"""
        test_cases = [
            (0.5, True),   # Valid size
            (1.0, True),   # Max size
            (1.5, False),  # Exceeds max
            (0, False),    # Zero size
            (-0.5, False), # Negative size
            (None, False)  # Invalid size
        ]
        
        def mock_validate_position_size(size):
            if size is None or size <= 0:
                return False
            return size <= risk_agent.config['max_position_size']
        
        risk_agent.validate_position_size = mock_validate_position_size
        
        for size, expected in test_cases:
            result = risk_agent.validate_position_size(size)
            assert result == expected
    
    def test_token_blacklist_check(self, risk_agent):
        """Test: Token blacklist validation"""
        test_cases = [
            ('SOL', True),     # Valid token
            ('USDC', True),    # Valid token
            ('SCAM1', False),  # Blacklisted
            ('RUG2', False),   # Blacklisted
            ('', False),       # Empty token
            (None, False)      # None token
        ]
        
        def mock_check_token_blacklist(token):
            if not token:
                return False
            return token not in risk_agent.config['blacklisted_tokens']
        
        risk_agent.check_token_blacklist = mock_check_token_blacklist
        
        for token, expected in test_cases:
            result = risk_agent.check_token_blacklist(token)
            assert result == expected
    
    def test_daily_trade_limit(self, risk_agent):
        """Test: Daily trade limit enforcement"""
        # Test normal operation
        risk_agent.daily_trades = 25
        
        def mock_check_daily_limit():
            return risk_agent.daily_trades < risk_agent.config['max_daily_trades']
        
        risk_agent.check_daily_limit = mock_check_daily_limit
        
        assert risk_agent.check_daily_limit() is True
        
        # Test limit exceeded
        risk_agent.daily_trades = 51
        assert risk_agent.check_daily_limit() is False
    
    def test_comprehensive_risk_assessment(self, risk_agent):
        """Test: Complete risk assessment logic"""
        trade_request = {
            'token': 'SOL',
            'amount': 0.5,
            'liquidity': 50000,
            'volatility': 0.15
        }
        
        def mock_assess_risk(request):
            checks = {
                'position_size': request['amount'] <= risk_agent.config['max_position_size'],
                'token_valid': request['token'] not in risk_agent.config['blacklisted_tokens'],
                'liquidity_ok': request['liquidity'] >= risk_agent.config['min_liquidity'],
                'daily_limit': risk_agent.daily_trades < risk_agent.config['max_daily_trades'],
                'volatility_ok': request['volatility'] < 0.5  # Max 50% volatility
            }
            
            all_passed = all(checks.values())
            
            return {
                'decision': 'GO' if all_passed else 'NO-GO',
                'checks': checks,
                'risk_score': sum(checks.values()) / len(checks)
            }
        
        risk_agent.assess_risk = mock_assess_risk
        
        result = risk_agent.assess_risk(trade_request)
        
        assert result['decision'] == 'GO'
        assert result['risk_score'] == 1.0  # All checks passed
        assert all(result['checks'].values())


class TestExecutorAgent:
    """Comprehensive unit tests for Executor Agent"""
    
    @pytest.fixture
    def executor_agent(self):
        """Create Executor Agent instance for testing"""
        executor = MagicMock()
        executor.wallet = MagicMock()
        executor.solana_client = AsyncMock()
        executor.config = {
            'max_slippage': 0.05,  # 5%
            'transaction_timeout': 30,
            'retry_attempts': 3
        }
        return executor
    
    @pytest.mark.asyncio
    async def test_transaction_building(self, executor_agent):
        """Test: Transaction building logic"""
        trade_order = {
            'action': 'BUY',
            'token': 'SOL',
            'amount': 0.5,
            'max_slippage': 0.03
        }
        
        mock_transaction = {
            'from': 'wallet_address',
            'to': 'dex_address',
            'amount': 0.5,
            'slippage': 0.03,
            'signature': None
        }
        
        async def mock_build_transaction(order):
            return {
                'transaction': mock_transaction,
                'estimated_gas': 5000,
                'success': True
            }
        
        executor_agent.build_transaction = mock_build_transaction
        
        result = await executor_agent.build_transaction(trade_order)
        
        assert result['success'] is True
        assert result['transaction']['amount'] == 0.5
        assert result['transaction']['slippage'] == 0.03
    
    @pytest.mark.asyncio
    async def test_transaction_retry_logic(self, executor_agent):
        """Test: Transaction retry mechanism"""
        attempt_count = 0
        
        async def mock_execute_with_retry(transaction):
            nonlocal attempt_count
            attempt_count += 1
            
            if attempt_count < 3:  # Fail first 2 attempts
                raise Exception(f"Network error (attempt {attempt_count})")
            
            return {'success': True, 'tx_hash': 'mock_hash'}
        
        executor_agent.execute_transaction = mock_execute_with_retry
        
        result = await executor_agent.execute_transaction({'mock': 'transaction'})
        
        assert result['success'] is True
        assert attempt_count == 3  # Should retry 3 times


# Integration test for A2A Protocol
class TestA2AProtocol:
    """Unit tests for A2A Protocol communication"""
    
    @pytest.mark.asyncio
    async def test_message_serialization(self):
        """Test: JSON-RPC message serialization"""
        message = {
            'method': 'publish_opportunity',
            'params': {
                'opportunity_id': 'test_123',
                'score': 25,
                'timestamp': datetime.now().isoformat()
            },
            'id': 1
        }
        
        # Test serialization
        serialized = json.dumps(message)
        assert isinstance(serialized, str)
        
        # Test deserialization
        deserialized = json.loads(serialized)
        assert deserialized['method'] == 'publish_opportunity'
        assert deserialized['params']['score'] == 25
    
    @pytest.mark.asyncio
    async def test_message_validation(self):
        """Test: A2A message validation"""
        valid_message = {
            'method': 'analyze_opportunity',
            'params': {'data': 'test'},
            'id': 1
        }
        
        invalid_messages = [
            {},  # Empty message
            {'method': 'test'},  # Missing params and id
            {'params': {}, 'id': 1},  # Missing method
            {'method': '', 'params': {}, 'id': 1}  # Empty method
        ]
        
        def validate_a2a_message(msg):
            required_fields = ['method', 'params', 'id']
            return all(field in msg and msg[field] is not None for field in required_fields)
        
        # Valid message should pass
        assert validate_a2a_message(valid_message) is True
        
        # Invalid messages should fail
        for invalid_msg in invalid_messages:
            assert validate_a2a_message(invalid_msg) is False


if __name__ == "__main__":
    # Run unit tests with coverage
    pytest.main([
        __file__,
        "-v",
        "--cov=agents",
        "--cov-report=html",
        "--cov-report=term-missing",
        "--cov-fail-under=95"
    ])
